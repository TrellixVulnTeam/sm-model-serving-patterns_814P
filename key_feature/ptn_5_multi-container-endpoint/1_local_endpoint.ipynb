{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea980e9",
   "metadata": {},
   "source": [
    "# Module 1. Check Inference Results & Local Mode Deployment\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "본 핸즈온은 AWS AIML Blog의 내용을 기반으로 MNIST 예제 대신 좀 더 실용적인 한국어 자연어 처리 예시를 다루며, 총 3종류(Sentiment Classification, KorSTS, KoBART)의 자연어 처리 모델을 SageMaker 다중 컨테이너 엔드포인트(Multi-container endpoint)로 배포하는 법을 익혀 봅니다.\n",
    "\n",
    "이미 SageMaker 기본 개념(로컬 모드, 호스팅 엔드포인트)과 자연어 처리 & Huggingface을 다뤄 보신 분들은 이 섹션을 건너 뛰고 다음 노트북으로 진행하셔도 됩니다.\n",
    "\n",
    "### References\n",
    "- AWS AIML Blog: https://aws.amazon.com/ko/blogs/machine-learning/deploy-multiple-serving-containers-on-a-single-instance-using-amazon-sagemaker-multi-container-endpoints/\n",
    "- Developer Guide: https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9e7efda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting PyYAML\n",
      "  Using cached PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[33mWARNING: Error parsing requirements for pyyaml: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/PyYAML-6.0.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: PyYAML\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "awscli 1.22.20 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\n",
      "Successfully installed PyYAML-6.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.12.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (4.12.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (4.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (1.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (0.2.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (3.3.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (2021.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (21.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (2.26.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.5) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers==4.12.5) (3.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (8.0.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU sagemaker botocore boto3 awscli\n",
    "!pip install --ignore-installed PyYAML\n",
    "!pip install transformers==4.12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c57fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import boto3\n",
    "import sagemaker\n",
    "import datetime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from src.utils import print_outputs, prepare_model_artifact, NLPPredictor \n",
    "\n",
    "role = get_execution_role()\n",
    "boto_session = boto3.session.Session()\n",
    "sm_session = sagemaker.session.Session()\n",
    "sm_client = boto_session.client(\"sagemaker\")\n",
    "sm_runtime = boto_session.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc8e5c9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Check Inference Results & Debugging\n",
    "---\n",
    "\n",
    "로컬 엔드포인트나 호스팅 엔드포인트 배포 전, 로컬 환경 상에서 직접 추론을 수행하여 결과를 확인합니다. 참고로, SageMaker에서 TensorFlow를 제외한 머신 러닝 프레임워크 추론 컨테이너는 아래의 인터페이스를 사용합니다.\n",
    "\n",
    "#### Option 1.\n",
    "- `model_fn(model_dir)`: 네트워크 아키텍처를 정의하고 S3의 model_dir에 저장된 모델 아티팩트를 로드합니다.\n",
    "- `input_fn(request_body, content_type)`: 입력 데이터를 전처리합니다. (예: request_body로 전송된 bytearray 배열을 PIL.Image로 변환 수 cropping, resizing, normalization등의 전처리 수행). content_type은 입력 데이터 종류에 따라 다양하게 처리 가능합니다. (예: application/x-npy, application/json, application/csv 등)\n",
    "- `predict_fn(input_object, model)`: input_fn을 통해 들어온 데이터에 대해 추론을 수행합니다.\n",
    "- `output_fn(prediction, accept_type)`: predict_fn에서 받은 추론 결과를 추가 변환을 거쳐 프론트 엔드로 전송합니다.\n",
    "\n",
    "#### Option 2.\n",
    "- `model_fn(model_dir)`: 네트워크 아키텍처를 정의하고 S3의 model_dir에 저장된 모델 아티팩트를 로드합니다.\n",
    "- `transform_fn(model, request_body, content_type, accept_type)`: input_fn(), predict_fn(), output_fn()을 transform_fn()으로 통합할 수 있습니다.\n",
    "\n",
    "모델, 배포에 초점을 맞추기 위해 Huggingface에 등록된 `KoELECTRA-Small-v3` 모델을 기반으로 네이버 영화 리뷰 데이터셋과 KorSTS (Korean Semantic Textual Similarity) 데이터셋으로 파인 튜닝하였습니다. 파인 튜닝은 온프레미스나 Huggingface on SageMaker로 쉽게 수행 가능합니다. \n",
    "\n",
    "- KoELECTRA: https://github.com/monologg/KoELECTRA\n",
    "- Huggingface on Amazon SageMaker: https://huggingface.co/docs/sagemaker/main\n",
    "\n",
    "\n",
    "### Model A: Sentiment Classification\n",
    "\n",
    "네이버 영화 리뷰 데이터의 긍정/부정 판별 예시입니다. \n",
    "- Naver sentiment movie corpus: https://github.com/e9t/nsmc\n",
    "- 예시\n",
    "    - '이 영화는 최고의 영화입니다' => {\"predicted_label\": \"Pos\", \"score\": 0.96}\n",
    "    - '최악이에요. 배우의 연기력도 좋지 않고 내용도 너무 허접합니다' => {\"predicted_label\": \"Neg\", \"score\": 0.99}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93604fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nn\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ElectraConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ElectraModel, AutoTokenizer, ElectraTokenizer, ElectraForSequenceClassification\n",
      "\n",
      "logging.basicConfig(\n",
      "    level=logging.INFO, \n",
      "    \u001b[36mformat\u001b[39;49;00m=\u001b[33m'\u001b[39;49;00m\u001b[33m[\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m%(filename)s\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m%(lineno)d\u001b[39;49;00m\u001b[33m} \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    handlers=[\n",
      "        logging.FileHandler(filename=\u001b[33m'\u001b[39;49;00m\u001b[33mtmp.log\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "        logging.StreamHandler(sys.stdout)\n",
      "    ]\n",
      ")\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "max_seq_length = \u001b[34m128\u001b[39;49;00m\n",
      "classes = [\u001b[33m'\u001b[39;49;00m\u001b[33mNeg\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPos\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdaekeun-ml/koelectra-small-v3-nsmc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_path=\u001b[34mNone\u001b[39;49;00m):\n",
      "    \u001b[37m####\u001b[39;49;00m\n",
      "    \u001b[37m# If you have your own trained model\u001b[39;49;00m\n",
      "    \u001b[37m# Huggingface pre-trained model: 'monologg/koelectra-small-v3-discriminator'\u001b[39;49;00m\n",
      "    \u001b[37m####\u001b[39;49;00m\n",
      "    \u001b[37m#config = ElectraConfig.from_json_file(f'{model_path}/config.json')\u001b[39;49;00m\n",
      "    \u001b[37m#model = ElectraForSequenceClassification.from_pretrained(f'{model_path}/model.pth', config=config)\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Download model from the Huggingface hub\u001b[39;49;00m\n",
      "    model = ElectraForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdaekeun-ml/koelectra-small-v3-nsmc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)   \n",
      "    model.to(device)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(input_data, content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/jsonlines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m): \n",
      "    data_str = input_data.decode(\u001b[33m\"\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    jsonlines = data_str.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    transformed_inputs = []\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m jsonline \u001b[35min\u001b[39;49;00m jsonlines:\n",
      "        text = json.loads(jsonline)[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m]\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33minput text: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(text))          \n",
      "        encode_plus_token = tokenizer.encode_plus(\n",
      "            text,\n",
      "            max_length=max_seq_length,\n",
      "            add_special_tokens=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            return_token_type_ids=\u001b[34mFalse\u001b[39;49;00m,\n",
      "            padding=\u001b[33m\"\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            return_attention_mask=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            truncation=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        )\n",
      "        transformed_inputs.append(encode_plus_token)\n",
      "        \n",
      "    \u001b[34mreturn\u001b[39;49;00m transformed_inputs\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(transformed_inputs, model):\n",
      "    predicted_classes = []\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m data \u001b[35min\u001b[39;49;00m transformed_inputs:\n",
      "        data = data.to(device)\n",
      "        output = model(**data)\n",
      "\n",
      "        softmax_fn = nn.Softmax(dim=\u001b[34m1\u001b[39;49;00m)\n",
      "        softmax_output = softmax_fn(output[\u001b[34m0\u001b[39;49;00m])\n",
      "        _, prediction = torch.max(softmax_output, dim=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "        predicted_class_idx = prediction.item()\n",
      "        predicted_class = classes[predicted_class_idx]\n",
      "        score = softmax_output[\u001b[34m0\u001b[39;49;00m][predicted_class_idx]\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mpredicted_class: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(predicted_class))\n",
      "\n",
      "        prediction_dict = {}\n",
      "        prediction_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mpredicted_label\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = predicted_class\n",
      "        prediction_dict[\u001b[33m'\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = score.cpu().detach().numpy().tolist()\n",
      "\n",
      "        jsonline = json.dumps(prediction_dict)\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mjsonline: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(jsonline))        \n",
      "        predicted_classes.append(jsonline)\n",
      "\n",
      "    predicted_classes_jsonlines = \u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(predicted_classes)\n",
      "    \u001b[34mreturn\u001b[39;49;00m predicted_classes_jsonlines\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(outputs, accept=\u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/jsonlines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "    \u001b[34mreturn\u001b[39;49;00m outputs, accept\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/inference_nsmc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b98b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c190b1d4d4543e696f7550e4840765c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caa92b652a243eaa26fb4dff4ddef1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/257k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c7f644ce76486e9281d86b5e0cb3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/521k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3990db43f1c94d848aec18b2fa4d1682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{inference_nsmc.py:47} INFO - input text: 이 영화는 최고의 영화입니다\n",
      "[{inference_nsmc.py:47} INFO - input text: 최악이에요. 배우의 연기력도 좋지 않고 내용도 너무 허접합니다\n",
      "[{inference_nsmc.py:77} INFO - predicted_class: Pos\n",
      "[{inference_nsmc.py:84} INFO - jsonline: {\"predicted_label\": \"Pos\", \"score\": 0.9619030952453613}\n",
      "[{inference_nsmc.py:77} INFO - predicted_class: Neg\n",
      "[{inference_nsmc.py:84} INFO - jsonline: {\"predicted_label\": \"Neg\", \"score\": 0.9994170665740967}\n",
      "{\"predicted_label\": \"Pos\", \"score\": 0.9619030952453613}\n",
      "{\"predicted_label\": \"Neg\", \"score\": 0.9994170665740967}\n"
     ]
    }
   ],
   "source": [
    "from src.inference_nsmc import model_fn, input_fn, predict_fn, output_fn\n",
    "modelA_path = 'model-nsmc'\n",
    "\n",
    "with open('samples/nsmc.txt', mode='rb') as file:\n",
    "    modelA_input_data = file.read()\n",
    "\n",
    "modelA = model_fn(modelA_path)\n",
    "transformed_inputs = input_fn(modelA_input_data)\n",
    "predicted_classes_jsonlines = predict_fn(transformed_inputs, modelA)\n",
    "modelA_outputs = output_fn(predicted_classes_jsonlines)\n",
    "print(modelA_outputs[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbcc5f",
   "metadata": {},
   "source": [
    "### Model B: Semantic Textual Similarity (STS)\n",
    "\n",
    "두 문장간의 유사도를 정량화하는 예시입니다.\n",
    "- KorNLI and KorSTS: https://github.com/kakaobrain/KorNLUDatasets\n",
    "- 예시\n",
    "    - ['맛있는 라면을 먹고 싶어요', '후루룩 쩝쩝 후루룩 쩝쩝 맛좋은 라면'] => {\"score\": 4.78}\n",
    "    - ['뽀로로는 내친구', '머신러닝은 러닝머신이 아닙니다.'] => {\"score\": 0.23}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c595432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nn\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ElectraConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ElectraModel, AutoTokenizer, ElectraTokenizer, ElectraForSequenceClassification\n",
      "\n",
      "logging.basicConfig(\n",
      "    level=logging.INFO, \n",
      "    \u001b[36mformat\u001b[39;49;00m=\u001b[33m'\u001b[39;49;00m\u001b[33m[\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m%(filename)s\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m%(lineno)d\u001b[39;49;00m\u001b[33m} \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    handlers=[\n",
      "        logging.FileHandler(filename=\u001b[33m'\u001b[39;49;00m\u001b[33mtmp.log\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "        logging.StreamHandler(sys.stdout)\n",
      "    ]\n",
      ")\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "max_seq_length = \u001b[34m128\u001b[39;49;00m\n",
      "tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdaekeun-ml/koelectra-small-v3-korsts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[37m# Huggingface pre-trained model: 'monologg/koelectra-small-v3-discriminator'\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_path=\u001b[34mNone\u001b[39;49;00m):\n",
      "    \u001b[37m####\u001b[39;49;00m\n",
      "    \u001b[37m# If you have your own trained model\u001b[39;49;00m\n",
      "    \u001b[37m# Huggingface pre-trained model: 'monologg/koelectra-small-v3-discriminator'\u001b[39;49;00m\n",
      "    \u001b[37m####    \u001b[39;49;00m\n",
      "    \u001b[37m#config = ElectraConfig.from_json_file(f'{model_path}/config.json')\u001b[39;49;00m\n",
      "    \u001b[37m#model = ElectraForSequenceClassification.from_pretrained(f'{model_path}/model.pth', config=config)\u001b[39;49;00m\n",
      "    model = ElectraForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdaekeun-ml/koelectra-small-v3-korsts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    model.to(device)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(input_data, content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/jsonlines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "    data_str = input_data.decode(\u001b[33m\"\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    jsonlines = data_str.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    transformed_inputs = []\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m jsonline \u001b[35min\u001b[39;49;00m jsonlines:\n",
      "        text = json.loads(jsonline)[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33minput text: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(text))          \n",
      "        encode_plus_token = tokenizer.encode_plus(\n",
      "            text,\n",
      "            max_length=max_seq_length,\n",
      "            add_special_tokens=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            return_token_type_ids=\u001b[34mFalse\u001b[39;49;00m,\n",
      "            padding=\u001b[33m\"\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            return_attention_mask=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            truncation=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        )\n",
      "        transformed_inputs.append(encode_plus_token)\n",
      "        \n",
      "    \u001b[34mreturn\u001b[39;49;00m transformed_inputs\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(transformed_inputs, model):\n",
      "    predicted_classes = []\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m data \u001b[35min\u001b[39;49;00m transformed_inputs:\n",
      "        data = data.to(device)\n",
      "        output = model(**data)\n",
      "\n",
      "        prediction_dict = {}\n",
      "        prediction_dict[\u001b[33m'\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = output[\u001b[34m0\u001b[39;49;00m].squeeze().cpu().detach().numpy().tolist()\n",
      "\n",
      "        jsonline = json.dumps(prediction_dict)\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mjsonline: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(jsonline))        \n",
      "        predicted_classes.append(jsonline)\n",
      "\n",
      "    predicted_classes_jsonlines = \u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(predicted_classes)\n",
      "    \u001b[34mreturn\u001b[39;49;00m predicted_classes_jsonlines\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(outputs, accept=\u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/jsonlines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "    \u001b[34mreturn\u001b[39;49;00m outputs, accept\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/inference_korsts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc27560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c71b28242541769fae3ebd908a6d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8c89d322e5487d9800776350f9589c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/257k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc32a1e49c90486da0f9b27ccbb5cadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/521k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace53bd6504a42758b8d677125d89e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1127f748854151b1e170543e16dbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/885 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d7ce1ec2884042bbb21d2da629e49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/54.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{inference_korsts.py:44} INFO - input text: ['맛있는 라면을 먹고 싶어요', '후루룩 쩝쩝 후루룩 쩝쩝 맛좋은 라면']\n",
      "[{inference_korsts.py:44} INFO - input text: ['뽀로로는 내친구', '머신러닝은 러닝머신이 아닙니다.']\n",
      "[{inference_korsts.py:71} INFO - jsonline: {\"score\": 4.786738872528076}\n",
      "[{inference_korsts.py:71} INFO - jsonline: {\"score\": 0.2319067120552063}\n",
      "{\"score\": 4.786738872528076}\n",
      "{\"score\": 0.2319067120552063}\n"
     ]
    }
   ],
   "source": [
    "from src.inference_korsts import model_fn, input_fn, predict_fn, output_fn\n",
    "modelB_path = 'model-korsts'\n",
    "\n",
    "with open('samples/korsts.txt', mode='rb') as file:\n",
    "    modelB_input_data = file.read()    \n",
    "    \n",
    "modelB = model_fn(modelB_path)\n",
    "transformed_inputs = input_fn(modelB_input_data)\n",
    "predicted_classes_jsonlines = predict_fn(transformed_inputs, modelB)\n",
    "modelB_outputs = output_fn(predicted_classes_jsonlines)\n",
    "print(modelB_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9bf54",
   "metadata": {},
   "source": [
    "### Model C: KoBART (Korean Bidirectional and Auto-Regressive Transformers)\n",
    "\n",
    "문서 내용(예: 뉴스 기사)을 요약하는 예시입니다.\n",
    "\n",
    "- KoBART: https://github.com/SKT-AI/KoBART\n",
    "- KoBART Summarization: https://github.com/seujung/KoBART-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4583954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nn\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PreTrainedTokenizerFast, BartForConditionalGeneration\n",
      "\n",
      "logging.basicConfig(\n",
      "    level=logging.INFO, \n",
      "    \u001b[36mformat\u001b[39;49;00m=\u001b[33m'\u001b[39;49;00m\u001b[33m[\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m%(filename)s\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m%(lineno)d\u001b[39;49;00m\u001b[33m} \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    handlers=[\n",
      "        logging.FileHandler(filename=\u001b[33m'\u001b[39;49;00m\u001b[33mtmp.log\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "        logging.StreamHandler(sys.stdout)\n",
      "    ]\n",
      ")\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "tokenizer = PreTrainedTokenizerFast.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mainize/kobart-news\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_path=\u001b[34mNone\u001b[39;49;00m):\n",
      "    model = BartForConditionalGeneration.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mainize/kobart-news\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model.to(device)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_fn\u001b[39;49;00m(model, input_data, content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/jsonlines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, accept=\u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/jsonlines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m): \n",
      "    data_str = input_data.decode(\u001b[33m\"\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    jsonlines = data_str.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    predicted = []\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m jsonline \u001b[35min\u001b[39;49;00m jsonlines:\n",
      "        text = json.loads(jsonline)[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m]\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33minput text: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(text))  \n",
      "        \n",
      "        input_ids = tokenizer.encode(text, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        input_ids = input_ids.to(device)        \n",
      "        \u001b[37m# Generate Summary Text Ids\u001b[39;49;00m\n",
      "        summary_text_ids = model.generate(\n",
      "            input_ids=input_ids,\n",
      "            bos_token_id=model.config.bos_token_id,\n",
      "            eos_token_id=model.config.eos_token_id,\n",
      "            length_penalty=\u001b[34m2.0\u001b[39;49;00m,\n",
      "            max_length=\u001b[34m512\u001b[39;49;00m,\n",
      "            min_length=\u001b[34m32\u001b[39;49;00m,\n",
      "            num_beams=\u001b[34m4\u001b[39;49;00m,\n",
      "        )        \n",
      "        \n",
      "        \u001b[37m# Decoding Text\u001b[39;49;00m\n",
      "        summary_outputs = tokenizer.decode(summary_text_ids[\u001b[34m0\u001b[39;49;00m], skip_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33msummary_outputs: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(summary_outputs))        \n",
      "        \n",
      "        prediction_dict = {}\n",
      "        prediction_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33msummary\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = summary_outputs\n",
      "\n",
      "        jsonline = json.dumps(prediction_dict)\n",
      "        predicted.append(jsonline)\n",
      "\n",
      "    predicted_jsonlines = \u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(predicted)\n",
      "    \u001b[34mreturn\u001b[39;49;00m predicted_jsonlines\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/inference_kobart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99336d",
   "metadata": {},
   "source": [
    "S3로 모델 아티팩트를 복사하는 대신 Huggingface에 등록된 모델을 그대로 사용합니다. model.pth는 0바이트의 빈 파일이며, 추론을 수행하기 위한 소스 코드들만 아카이빙됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fecd0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa567b9cb9d4402aa6ff38537aca0aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c63c01922a450a8c021fdaeccfd55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/302 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050a1d4498e541c791da52405608e78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d53167d78848b3887ba160d89ad48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b95a0a6fb6846788fa4a080044734ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{inference_kobart.py:36} INFO - input text: AWS가 10일 AWS 리인벤트를 통해 머신러닝 서비스인 아마존 세이지메이커(Amazon SageMaker)의 9가지 새로운 기능을 발표했다. 아마존 세이지메이커 데이터 랭글러(Amazon SageMaker Data Wrangler)를 비롯해 아마존 세이지메이커 피처 스토어 등 다양한 기술들이 속속 베일을 벗었다. 스와미 시바수브라마니안(Swami Sivasubramanian), AWS 아마존 머신러닝 부사장은 '수십만 명의 일반 개발자와 데이터 과학자가 업계 최고의 머신러닝 서비스인 아마존 세이지메이커를 활용해 맞춤형 머신러닝 모델 제작, 훈련 및 배치에 대한 장벽을 제거했다'면서 '개발자가 더 나은 가시성, 설명 가능성 및 자동화를 대규모로 구현하는 맞춤형 머신러닝 모델을 준비, 제작, 훈련, 설명, 검사, 모니터링, 디버그 및 실행하기 위한 엔드투엔드 머신러닝 파이프라인을 더 쉽게 구축할 수 있도록 지원한다'고 말했다.\n",
      "[{inference_kobart.py:53} INFO - summary_outputs: AWS가 10일 AWS 리인벤트를 통해 머신러닝 서비스인 아마존 세이지메이커 데이터 랭글러를 비롯한 아마존 세이지메이커 피처 스토어 등 9가지 새로운 기능을 발표하며 개발자가 더 나은 가시성, 설명 가능성 및 자동화를 대규모로 구현하는 맞춤형 머신러닝 모델을 준비, 제작, 훈련, 설명, 검사, 모니터링, 디버그 및 실행하기 위한 엔드투엔드 머신러닝 파이프라인을 더 쉽게 구축할 수 있도록 지원한다고 말했다.\n",
      "[{inference_kobart.py:36} INFO - input text: 대한항공은 이와 같은 필요성에 따라 AWS와 AWS의 국내 파트너사인 LG CNS와 함께 기존 사내 데이터 센터에서 운영했던 데이터와 네트워크, 보안 시스템을 비롯한 각종 IT시스템을 단계적으로 AWS의 클라우드로 이전해 효율성을 높이고 IT 관리를 단순화했다. 대한항공은 이번 전사 IT시스템의 클라우드 이전 완료에 따라 데이터 분석 능력, 머신러닝등 아마존웹서비스가 갖고 있는 클라우드 기능을 바탕으로 경영 프로세스 혁신, 여객서비스 강화, 예약·발권 시스템 편의성 증대, 기상예측 정확도 제고 등을 추진해 나간다. 대한항공은 먼저 ‘클라우드 머신러닝 관리 서비스’를 도입한다. 이는 머신러닝 모델의 구축, 학습, 적용을 모두 하나의 환경에서 관리할 수 있도록 해주는 서비스로 정확한 수요 및 통계 예측을 지원함으로써 보다 나은 고객 서비스를 제공할 수 있게 한다. 특히 악천후로 인한 항공기 지연 예상시간, 항공기 정비 소요시간 예측 등을 토대로 고객들에게 적절한 시점에 필요한 조치를 할 수 있을 것으로 기대된다. 또 AWS 클라우드로 구축된 고객 데이터 플랫폼에서 고객별 특성에 따른 고유 디지털 식별 정보가 부여돼, 맞춤형 고객 서비스 제공도 가능해질 것으로 보고있다. 다시 말해, 그 동안 고객이 대한항공으로부터 제공 받은 서비스를 포함한 각종 정보들을 종합적으로 분석해 고객 니즈에 맞는 맞춤 서비스를 추천하는 기능도 제공된다는 것이다.\n",
      "[{inference_kobart.py:53} INFO - summary_outputs: 대한항공은 사내 사내 데이터 센터에서 운영했던 데이터와 네트워크, 보안 시스템을 비롯한 각종 IT시스템을 단계적으로 AWS의 클라우드로 이전해 효율성을 높이고 IT 관리를 단순화했으며 클라우드 머신러닝 관리 서비스’를 도입하여 악천후로 인한 항공기 지연 예상시간, 정비 소요시간 예측 등을 토대로 적절한 시점에 필요한 조치를 할 수 있을 것으로 기대된다.\n"
     ]
    }
   ],
   "source": [
    "from src.inference_kobart import model_fn, transform_fn\n",
    "modelC_path = 'model-kobart'\n",
    "f = open(f\"{modelC_path}/model.pth\", 'w')\n",
    "f.close()\n",
    "\n",
    "with open('samples/kobart.txt', mode='rb') as file:\n",
    "    modelC_input_data = file.read()\n",
    "\n",
    "modelC = model_fn('./')\n",
    "outputs = transform_fn(modelC, modelC_input_data)\n",
    "\n",
    "with open('samples/kobart.txt', mode='rb') as file:\n",
    "    modelC_input_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f07f37",
   "metadata": {},
   "source": [
    "결괏값들을 확인했다면 로컬 모드로 빠르게 배포하여 테스트하는 것을 권장드립니다. 단, SageMaker Studio는 로컬 모드를 지원하지 않기 때문에 아래 섹션은 SageMaker에서 실행해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c7d6e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. (SageMaker Only) Local Mode Deployment for Model A\n",
    "---\n",
    "\n",
    "### Deploy Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA_artifact_name = 'modelA.tar.gz'\n",
    "prepare_model_artifact(modelA_path, model_artifact_name=modelA_artifact_name)\n",
    "local_model_path = f'file://{os.getcwd()}/{modelA_artifact_name}'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=local_model_path,\n",
    "    role=role,\n",
    "    entry_point='inference_nsmc.py', \n",
    "    source_dir='src',\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py3',\n",
    "    predictor_cls=NLPPredictor,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a4373",
   "metadata": {},
   "source": [
    "### Invoke using SageMaker Python SDK\n",
    "SageMaker SDK `predict()` 메서드로 간단하게 추론을 실행할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c40c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{\"text\": [\"이 영화는 최고의 영화입니다\"]}, \n",
    "          {\"text\": [\"최악이에요. 배우의 연기력도 좋지 않고 내용도 너무 허접합니다\"]}]\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd188795",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in predicted_classes:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f388bf9",
   "metadata": {},
   "source": [
    "### Invoke using Boto3 API\n",
    "이번에는 boto3의 `invoke_endpoint()` 메서드로 추론을 수행해 보겠습니다.\n",
    "Boto3는 서비스 레벨의 low-level SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 high-level SDK인 SageMaker SDK와 달리 SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d342a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sm_runtime = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = model.endpoint_name\n",
    "\n",
    "response = local_sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/jsonlines',\n",
    "    Accept='application/jsonlines',\n",
    "    Body=modelA_input_data\n",
    "    )\n",
    "outputs = response['Body'].read().decode()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputs(outputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f187c8",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있습니다.\n",
    "참고로, 노트북 인스턴스에서 추론 컨테이너를 배포했기 때문에 엔드포인트를 띄워 놓아도 별도로 추가 요금이 과금되지는 않습니다.\n",
    "\n",
    "로컬 엔드포인트는 도커 컨테이너이기 때문에 `docker rm $(docker ps -a -q)` 으로도 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc86873",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b8f8d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. (SageMaker Only) Local Mode Deployment for Model B\n",
    "---\n",
    "\n",
    "### Deploy Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB_artifact_name = 'modelB.tar.gz'\n",
    "prepare_model_artifact(modelB_path, model_artifact_name=modelB_artifact_name)\n",
    "local_model_path = f'file://{os.getcwd()}/{modelB_artifact_name}'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=local_model_path,\n",
    "    role=role,\n",
    "    entry_point='inference_korsts.py', \n",
    "    source_dir='src',\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py3',\n",
    "    predictor_cls=NLPPredictor,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ba0de",
   "metadata": {},
   "source": [
    "### Invoke using SageMaker Python SDK\n",
    "SageMaker SDK `predict()` 메서드로 간단하게 추론을 실행할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ae317",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{\"text\": [\"맛있는 라면을 먹고 싶어요\", \"후루룩 쩝쩝 후루룩 쩝쩝 맛좋은 라면\"]}, \n",
    "          {\"text\": [\"뽀로로는 내친구\", \"머신러닝은 러닝머신이 아닙니다.\"]}]\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in predicted_classes:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06efb519",
   "metadata": {},
   "source": [
    "### Invoke using Boto3 API\n",
    "이번에는 boto3의 `invoke_endpoint()` 메서드로 추론을 수행해 보겠습니다.\n",
    "Boto3는 서비스 레벨의 low-level SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 high-level SDK인 SageMaker SDK와 달리 SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sm_runtime = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = model.endpoint_name\n",
    "\n",
    "response = local_sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/jsonlines',\n",
    "    Accept='application/jsonlines',\n",
    "    Body=modelB_input_data\n",
    "    )\n",
    "outputs = response['Body'].read().decode()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b969b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad246e",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있습니다.\n",
    "참고로, 노트북 인스턴스에서 추론 컨테이너를 배포했기 때문에 엔드포인트를 띄워 놓아도 별도로 추가 요금이 과금되지는 않습니다.\n",
    "\n",
    "로컬 엔드포인트는 도커 컨테이너이기 때문에 `docker rm $(docker ps -a -q)` 으로도 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fa05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac9766",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. (SageMaker Only) Local Mode Deployment for Model C\n",
    "---\n",
    "\n",
    "### Deploy Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e000346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: omitting directory ‘./src/__pycache__’\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./model.pth\n",
      "./code/\n",
      "./code/inference_kobart.py\n",
      "./code/utils.py\n",
      "./code/inference_nsmc.py\n",
      "./code/requirements.txt\n",
      "./code/inference_korsts.py\n",
      "Archived modelC.tar.gz\n",
      "[{session.py:2706} INFO - Creating model with name: pytorch-inference-2022-01-20-09-09-04-955\n",
      "[{session.py:3066} INFO - Creating endpoint with name pytorch-inference-2022-01-20-09-09-04-956\n",
      "[{image.py:268} INFO - serving\n",
      "[{image.py:271} INFO - creating hosting dir in /tmp/tmpqyo55rd6\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[{image.py:1011} INFO - No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "[{image.py:683} INFO - docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-uwzc5:\n",
      "    command: serve\n",
      "    container_name: 775yc8w3lr-algo-1-uwzc5\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.7.1-cpu-py3\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-uwzc5\n",
      "    ports:\n",
      "    - 8080:8080\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpjqt4s2td:/opt/ml/model\n",
      "version: '2.3'\n",
      "\n",
      "[{image.py:706} INFO - docker command: docker-compose -f /tmp/tmpqyo55rd6/docker-compose.yaml up --build --abort-on-container-exit\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 5\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe0a95e3110>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe0a95e3790>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe0a965d790>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "Attaching to 775yc8w3lr-algo-1-uwzc5\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Collecting transformers==4.11.3\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 27.1 MB/s eta 0:00:01\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m \u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (4.59.0)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (5.4.1)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Collecting huggingface-hub>=0.0.17\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 7.9 MB/s  eta 0:00:01\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 10\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m \u001b[?25hCollecting regex!=2019.12.17\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading regex-2022.1.18-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "\u001b[K     |████████████████████████████████| 748 kB 61.0 MB/s eta 0:00:01\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m \u001b[?25hRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Collecting importlib-metadata\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 67.9 MB/s eta 0:00:01\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m \u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Collecting sacremoses\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 73.0 MB/s eta 0:00:01\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m \u001b[?25hCollecting filelock\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (3.10.0.0)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Collecting packaging>=20.0\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 9.0 MB/s  eta 0:00:01\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m \u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.4.7)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Collecting zipp>=0.5\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2020.12.5)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Collecting click\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m \u001b[?25hInstalling collected packages: zipp, importlib-metadata, regex, packaging, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m   Attempting uninstall: packaging\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m     Found existing installation: packaging 20.4\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m     Uninstalling packaging-20.4:\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m       Successfully uninstalled packaging-20.4\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Successfully installed click-8.0.3 filelock-3.4.1 huggingface-hub-0.4.0 importlib-metadata-4.8.3 packaging-21.3 regex-2022.1.18 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.11.3 zipp-3.6.0\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m \u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,359 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Torchserve version: 0.3.1\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m TS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Current directory: /\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Number of CPUs: 4\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Max heap size: 3936 M\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Python executable: /opt/conda/bin/python3.6\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Log dir: /logs\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Netty threads: 0\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Netty client threads: 0\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Default workers per model: 4\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m Enable metrics API: true\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,405 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,427 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 270fe1cd736c409c8299d6ce0b2afdf3\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,438 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,472 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,658 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,660 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]60\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,660 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,660 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,673 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,688 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,689 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,689 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,690 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]57\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,691 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,691 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,693 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]58\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,694 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,694 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,694 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]59\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,694 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,694 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,694 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,693 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,694 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,710 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,710 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,711 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,717 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,718 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,717 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,726 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 15\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,903 [INFO ] pool-1-thread-5 ACCESS_LOG - /172.19.0.1:41002 \"GET /ping HTTP/1.1\" 200 52\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:15,905 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bed9bb5e06f2,timestamp:null\n",
      "!"
     ]
    }
   ],
   "source": [
    "modelC_artifact_name = 'modelC.tar.gz'\n",
    "prepare_model_artifact(modelC_path, model_artifact_name=modelC_artifact_name)\n",
    "local_model_path = f'file://{os.getcwd()}/{modelC_artifact_name}'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=local_model_path,\n",
    "    role=role,\n",
    "    entry_point='inference_kobart.py', \n",
    "    source_dir='src',\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py3',\n",
    "    predictor_cls=NLPPredictor,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54f1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,328 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12501\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,328 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12504\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,328 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12498\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,337 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12515\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,338 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:12886|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669768\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,338 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:12889|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669768\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,338 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:12886|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669768\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,339 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:109|#Level:Host|#hostname:bed9bb5e06f2,timestamp:null\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,338 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:12886|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669768\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,339 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:bed9bb5e06f2,timestamp:null\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,339 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:105|#Level:Host|#hostname:bed9bb5e06f2,timestamp:null\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:28,339 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:103|#Level:Host|#hostname:bed9bb5e06f2,timestamp:null\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710776eb",
   "metadata": {},
   "source": [
    "### Invoke using Boto3 API\n",
    "**[주의]** BART 모델은 Auto-Regressive 모델로 내부적으로 연산을 많이 수행하여 기본 인스턴스(예: `ml.t2.medium`)를 사용하는 경우, 시간이 상대적으로 오래 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d769ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:31,542 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - input text: AWS가 10일 AWS 리인벤트를 통해 머신러닝 서비스인 아마존 세이지메이커(Amazon SageMaker)의 9가지 새로운 기능을 발표했다. 아마존 세이지메이커 데이터 랭글러(Amazon SageMaker Data Wrangler)를 비롯해 아마존 세이지메이커 피처 스토어 등 다양한 기술들이 속속 베일을 벗었다. 스와미 시바수브라마니안(Swami Sivasubramanian), AWS 아마존 머신러닝 부사장은 '수십만 명의 일반 개발자와 데이터 과학자가 업계 최고의 머신러닝 서비스인 아마존 세이지메이커를 활용해 맞춤형 머신러닝 모델 제작, 훈련 및 배치에 대한 장벽을 제거했다'면서 '개발자가 더 나은 가시성, 설명 가능성 및 자동화를 대규모로 구현하는 맞춤형 머신러닝 모델을 준비, 제작, 훈련, 설명, 검사, 모니터링, 디버그 및 실행하기 위한 엔드투엔드 머신러닝 파이프라인을 더 쉽게 구축할 수 있도록 지원한다'고 말했다.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:35,800 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - summary_outputs: AWS가 10일 AWS 리인벤트를 통해 머신러닝 서비스인 아마존 세이지메이커 데이터 랭글러를 비롯한 아마존 세이지메이커 피처 스토어 등 9가지 새로운 기능을 발표하며 개발자가 더 나은 가시성, 설명 가능성 및 자동화를 대규모로 구현하는 맞춤형 머신러닝 모델을 준비, 제작, 훈련, 설명, 검사, 모니터링, 디버그 및 실행하기 위한 엔드투엔드 머신러닝 파이프라인을 더 쉽게 구축할 수 있도록 지원한다고 말했다.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:35,800 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - input text: 대한항공은 이와 같은 필요성에 따라 AWS와 AWS의 국내 파트너사인 LG CNS와 함께 기존 사내 데이터 센터에서 운영했던 데이터와 네트워크, 보안 시스템을 비롯한 각종 IT시스템을 단계적으로 AWS의 클라우드로 이전해 효율성을 높이고 IT 관리를 단순화했다. 대한항공은 이번 전사 IT시스템의 클라우드 이전 완료에 따라 데이터 분석 능력, 머신러닝등 아마존웹서비스가 갖고 있는 클라우드 기능을 바탕으로 경영 프로세스 혁신, 여객서비스 강화, 예약·발권 시스템 편의성 증대, 기상예측 정확도 제고 등을 추진해 나간다. 대한항공은 먼저 ‘클라우드 머신러닝 관리 서비스’를 도입한다. 이는 머신러닝 모델의 구축, 학습, 적용을 모두 하나의 환경에서 관리할 수 있도록 해주는 서비스로 정확한 수요 및 통계 예측을 지원함으로써 보다 나은 고객 서비스를 제공할 수 있게 한다. 특히 악천후로 인한 항공기 지연 예상시간, 항공기 정비 소요시간 예측 등을 토대로 고객들에게 적절한 시점에 필요한 조치를 할 수 있을 것으로 기대된다. 또 AWS 클라우드로 구축된 고객 데이터 플랫폼에서 고객별 특성에 따른 고유 디지털 식별 정보가 부여돼, 맞춤형 고객 서비스 제공도 가능해질 것으로 보고있다. 다시 말해, 그 동안 고객이 대한항공으로부터 제공 받은 서비스를 포함한 각종 정보들을 종합적으로 분석해 고객 니즈에 맞는 맞춤 서비스를 추천하는 기능도 제공된다는 것이다.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:38,761 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - summary_outputs: 대한항공은 사내 사내 데이터 센터에서 운영했던 데이터와 네트워크, 보안 시스템을 비롯한 각종 IT시스템을 단계적으로 AWS의 클라우드로 이전해 효율성을 높이고 IT 관리를 단순화했으며 클라우드 머신러닝 관리 서비스’를 도입하여 악천후로 인한 항공기 지연 예상시간, 정비 소요시간 예측 등을 토대로 적절한 시점에 필요한 조치를 할 수 있을 것으로 기대된다.\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:38,763 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 7225\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:38,763 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:7221.08|#ModelName:model,Level:Model|#hostname:bed9bb5e06f2,requestID:2f350209-3048-4d65-a931-c8ba0b80ba8e,timestamp:1642669778\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:38,764 [INFO ] W-9000-model_1 ACCESS_LOG - /172.19.0.1:42410 \"POST /invocations HTTP/1.1\" 200 7238\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:38,765 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:bed9bb5e06f2,timestamp:null\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:38,766 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:bed9bb5e06f2,timestamp:null\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:09:38,766 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:bed9bb5e06f2,timestamp:null\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:10:16,603 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669816\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:10:16,604 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:69.69393539428711|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669816\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:10:16,604 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.59603500366211|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669816\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:10:16,604 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:25.3|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669816\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:10:16,604 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1950.19921875|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669816\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:10:16,604 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:13454.3046875|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669816\n",
      "\u001b[36m775yc8w3lr-algo-1-uwzc5 |\u001b[0m 2022-01-20 09:10:16,604 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:87.6|#Level:Host|#hostname:bed9bb5e06f2,timestamp:1642669816\n"
     ]
    }
   ],
   "source": [
    "local_sm_runtime = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = model.endpoint_name\n",
    "\n",
    "response = local_sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/jsonlines',\n",
    "    Accept='application/jsonlines',\n",
    "    Body=modelC_input_data\n",
    "    )\n",
    "outputs = response['Body'].read().decode()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cff79",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있습니다.\n",
    "참고로, 노트북 인스턴스에서 추론 컨테이너를 배포했기 때문에 엔드포인트를 띄워 놓아도 별도로 추가 요금이 과금되지는 않습니다.\n",
    "\n",
    "로컬 엔드포인트는 도커 컨테이너이기 때문에 `docker rm $(docker ps -a -q)` 으로도 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
